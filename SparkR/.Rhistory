source('~/git-review/SparkR/Learning.R')
source('~/git-review/SparkR/Learning.R')
source('~/git-review/SparkR/Learning.R')
source('~/git-review/SparkR/Learning.R')
DF <- createDataFrame(sqlContext, faithful)
head(DF)
localDF <- data.frame(name=c("John", "Smith", "Sarah"), age=c(19, 23, 18))
localDF
df <- createDataFrame(sqlContext, localDF)
df
head(df)
printSchema(df)
print(df)
s=collect(df)
s
source('~/git-review/SparkR/Learning.R')
Sys.getenv("SPARK_HOME")
source('~/git-review/cassandra_spark/SparkR/Learning.R')
source('~/git-review/cassandra_spark/SparkR/Learning.R')
sparkR.stop()
source('~/git-review/cassandra_spark/SparkR/Learning.R')
sparkR.stop()
source('~/git-review/cassandra_spark/SparkR/Learning.R')
sc <- sparkR.init(sparkPackages="com.databricks:spark-csv_2.11:1.0.3")
sqlContext = sparkRSQL.init(sc)
getwd()
setwd("/home/hui/spark-1.6.0-bin-hadoop2.6/")
getwd()
source('~/git-review/cassandra_spark/SparkR/Learning.R')
people <- read.df(sqlContext, "./examples/src/main/resources/people.json", "json")
sparkR.stop()
sc = sparkR.init(master="spark://192.168.0.3:7077", sparkEnvir = list(spark.driver.memory="1g"))
sqlContext = sparkRSQL.init(sc)
people <- read.df(sqlContext, "./examples/src/main/resources/people.json", "json")
people
head(people)
printSchema(people)
hiveContext <- sparkRHive.init(sc)
sql(hiveContext, "CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
sparkR.stop()
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
file.path(Sys.getenv("SPARK_HOME"), "R", "lib")
Sys.setenv(SPARK_HOME='C:/apps/spark-1.6.0-bin-hadoop2.6')
file.path(Sys.getenv("SPARK_HOME"), "R", "lib")
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
initSparkContext()
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
faiths = createDataFrame(sqlContext, faithful)
head(faiths)
sparkR.stop()
faiths = createDataFrame(sqlContext, faithful)
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
extraJarDir = "C:/apps/java_lib"
list.files(path=extraJarDir, pattern='*.jar')
x=list.files(path=extraJarDir, pattern='*.jar')
class(x)
x
x=list.files(path=extraJarDir, pattern='*.jar', full.names=TRUE)
x
paste0(x, collapse=':')
paste0(x, collapse=';')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
initSpark()
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
tryJDBC()
read.df
help(read.df)
help(read.jdbc)
help("sparkR.init")
help("read.df")
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
jars = list.files(path="C:/apps/java_lib", pattern='*.jar', full.names=TRUE)
extraCP <<- paste0(jars, collapse=';')
extraCP
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
JDBC(driverClass="com.vertica.jdbc.Driver", classPath=extraCP)
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
v1=verticaJDBC()
dd=dbGetQuery(v1, "select * from PTG_WS.US_STATE")
dd
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
df= createDataFrame(sqContext, dd)
df= createDataFrame(sqlContext, dd)
printSchema(df)
sql(sqlContext, "select * from df where code='TX")
sql(sqlContext, "select * from df where CODE='TX")
sql(sqlContext, "select * from df where CODE='TX')
""
)
)
"
sql(sqlContext, "select * from df where CODE='TX'")
registerTempTable(df, "states")
sql(sqlContext, "select * from states where CODE='TX'")
sql(sqlContext, "select * from states where CODE='TX'")
x=sql(sqlContext, "select * from states where CODE='TX'")
x
x=sql(sqlContext, "select * from states")
x
head(x)
x=collect(sql(sqlContext, "select * from states"))
x
x=collect(sql(sqlContext, "select * from states where code='TX'"))
x=collect(sql(sqlContext, "select * from states where CODE='TX'"))
x
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
tryJDBC()
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
tryJDBC()
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
initSpark()
vertica = verticaJDBC()
sql = "select * from PTG_DWH.PTG_CAN_PRODUCT"
dd=dbGetQuery(vertica, sql)
sparkDF = createDataFrame(sqlContext, dd)
write.csv(dd, row.names=FALSE, file="1.csv")
read.df(sqlContext, '1.csv', source='com.databricks.spark.csv', header='true', inferSchema='true')
x=read.df(sqlContext, '1.csv', source='com.databricks.spark.csv', header='true', inferSchema='true')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
source('C:/cygwin64/home/Hui Wang/git-review/cassandra_spark/SparkR/Work_Learning.R')
stopSpark()
install.packages("stringr")
install.packages("stringi")
install.packages("foreign")
install.packages("RJDBC")
